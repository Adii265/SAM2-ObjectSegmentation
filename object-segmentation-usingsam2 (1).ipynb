{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9482162,"sourceType":"datasetVersion","datasetId":5767833},{"sourceId":9713655,"sourceType":"datasetVersion","datasetId":5942055},{"sourceId":90860,"sourceType":"modelInstanceVersion","modelInstanceId":76172,"modelId":100857}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/segment-anything-2.git\n%cd /kaggle/working/segment-anything-2\n%pip install -e .","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:56:20.735034Z","iopub.execute_input":"2024-10-26T04:56:20.735527Z","iopub.status.idle":"2024-10-26T04:58:41.899363Z","shell.execute_reply.started":"2024-10-26T04:56:20.735481Z","shell.execute_reply":"2024-10-26T04:58:41.897944Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Cloning into 'segment-anything-2'...\nremote: Enumerating objects: 974, done.\u001b[K\nremote: Counting objects: 100% (38/38), done.\u001b[K\nremote: Compressing objects: 100% (30/30), done.\u001b[K\nremote: Total 974 (delta 17), reused 21 (delta 7), pack-reused 936 (from 1)\u001b[K\nReceiving objects: 100% (974/974), 128.94 MiB | 38.65 MiB/s, done.\nResolving deltas: 100% (334/334), done.\n/kaggle/working/segment-anything-2\nObtaining file:///kaggle/working/segment-anything-2\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (2.4.0)\nRequirement already satisfied: torchvision>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (0.19.0)\nRequirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (1.26.4)\nRequirement already satisfied: tqdm>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (4.66.4)\nRequirement already satisfied: hydra-core>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (1.3.2)\nRequirement already satisfied: iopath>=0.1.10 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (0.1.10)\nRequirement already satisfied: pillow>=9.4.0 in /opt/conda/lib/python3.10/site-packages (from SAM-2==1.0) (10.3.0)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /opt/conda/lib/python3.10/site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.10/site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.10->SAM-2==1.0) (2.10.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.1->SAM-2==1.0) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.1->SAM-2==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.1->SAM-2==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.1->SAM-2==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.1->SAM-2==1.0) (2024.6.1)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.3.1->SAM-2==1.0) (2.1.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->hydra-core>=1.3.2->SAM-2==1.0) (3.1.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.3.1->SAM-2==1.0) (1.3.0)\nBuilding wheels for collected packages: SAM-2\n  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for SAM-2: filename=SAM_2-1.0-0.editable-py3-none-any.whl size=13393 sha256=3b76036f33b046260f84db8df889d228e27044c634e517c9f7f2cbd255e4ae93\n  Stored in directory: /tmp/pip-ephem-wheel-cache-l6g2x6lm/wheels/1b/30/72/94667a625e111d514f83ac75152c8957c764a2c56d72e883a6\nSuccessfully built SAM-2\nInstalling collected packages: SAM-2\n  Attempting uninstall: SAM-2\n    Found existing installation: SAM-2 1.0\n    Uninstalling SAM-2-1.0:\n      Successfully uninstalled SAM-2-1.0\nSuccessfully installed SAM-2-1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-base-plus/1\")\n\n!pip install modelbit\n\nimport os\nimport cv2\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sam2.build_sam import build_sam2\nfrom sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:41.901923Z","iopub.execute_input":"2024-10-26T04:58:41.902334Z","iopub.status.idle":"2024-10-26T04:58:55.593484Z","shell.execute_reply.started":"2024-10-26T04:58:41.902289Z","shell.execute_reply":"2024-10-26T04:58:55.591960Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: modelbit in /opt/conda/lib/python3.10/site-packages (0.42.0)\nRequirement already satisfied: pycryptodomex in /opt/conda/lib/python3.10/site-packages (from modelbit) (3.21.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from modelbit) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from modelbit) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from modelbit) (2.32.3)\nRequirement already satisfied: types-requests in /opt/conda/lib/python3.10/site-packages (from modelbit) (2.31.0.6)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from modelbit) (6.0.2)\nRequirement already satisfied: types-PyYAML in /opt/conda/lib/python3.10/site-packages (from modelbit) (6.0.12.20240917)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from modelbit) (3.1.4)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from modelbit) (0.23.0)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from modelbit) (1.4.4)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from modelbit) (1.7.0)\nRequirement already satisfied: build in /opt/conda/lib/python3.10/site-packages (from modelbit) (0.10.0)\nRequirement already satisfied: pkginfo in /opt/conda/lib/python3.10/site-packages (from modelbit) (1.11.2)\nRequirement already satisfied: boto3>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from modelbit) (1.26.100)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from modelbit) (1.26.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from modelbit) (7.0.0)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from modelbit) (1.26.18)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.23.0->modelbit) (1.29.165)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.23.0->modelbit) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.23.0->modelbit) (0.6.2)\nRequirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.10/site-packages (from build->modelbit) (21.3)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build->modelbit) (1.2.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build->modelbit) (2.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->modelbit) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->modelbit) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->modelbit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->modelbit) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->modelbit) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->modelbit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->modelbit) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->modelbit) (2024.8.30)\nRequirement already satisfied: types-urllib3 in /opt/conda/lib/python3.10/site-packages (from types-requests->modelbit) (1.26.25.14)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=19.0->build->modelbit) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->modelbit) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> > ","metadata":{}},{"cell_type":"code","source":"def save_image(image, filename):\n    cv2.imwrite(os.path.join(training_images_dir, filename), image)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.595346Z","iopub.execute_input":"2024-10-26T04:58:55.595760Z","iopub.status.idle":"2024-10-26T04:58:55.601824Z","shell.execute_reply.started":"2024-10-26T04:58:55.595715Z","shell.execute_reply":"2024-10-26T04:58:55.600715Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Function to extract objects from the provided images\ndef extract_objects_from_images(image_paths):\n    extracted_objects = []\n    \n    for img_path in image_paths:\n        # Load image\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        \n        # Generate masks\n        masks = mask_generator.generate(image)\n        \n        for mask in masks:\n            object_image = extract_object(image, mask['segmentation'])\n            extracted_objects.append(object_image)\n    \n    return extracted_objects","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.603344Z","iopub.execute_input":"2024-10-26T04:58:55.603805Z","iopub.status.idle":"2024-10-26T04:58:55.613031Z","shell.execute_reply.started":"2024-10-26T04:58:55.603736Z","shell.execute_reply":"2024-10-26T04:58:55.611726Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def extract_bounding_box(mask_list):\n    \"\"\"\n    Extract bounding box coordinates from a list of masks.\n    Returns list of (xmin, ymin, xmax, ymax) for each mask.\n    \"\"\"\n    bounding_boxes = []\n    for mask in mask_list:\n        binary_mask = mask.astype(np.uint8) * 255\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if len(contours) > 0:  # Check if contour exists\n            x, y, w, h = cv2.boundingRect(contours[0])\n            bounding_boxes.append((x, y, x + w, y + h))\n    return bounding_boxes","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.617291Z","iopub.execute_input":"2024-10-26T04:58:55.617905Z","iopub.status.idle":"2024-10-26T04:58:55.626137Z","shell.execute_reply.started":"2024-10-26T04:58:55.617846Z","shell.execute_reply":"2024-10-26T04:58:55.624940Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def track_object_with_bounding_box(object_image, target_image):\n    \"\"\"\n    Track the object in the target image by matching features and drawing a bounding box.\n    \n    Parameters:\n    - object_image: Extracted object from the first image.\n    - target_image: The second image where the object is searched.\n\n    Returns:\n    - Target image with the bounding box drawn.\n    \"\"\"\n    # Convert both images to grayscale\n    if object_image.size == 0 or target_image.size == 0:\n        print(\"Error: Empty image or object crop detected.\")\n        return target_image\n    gray_object = cv2.cvtColor(object_image, cv2.COLOR_RGB2GRAY)\n    gray_target = cv2.cvtColor(target_image, cv2.COLOR_RGB2GRAY)\n\n    # Initialize ORB detector for feature matching\n    orb = cv2.ORB_create(5000)  # Extract up to 5000 keypoints\n    kp1, des1 = orb.detectAndCompute(gray_object, None)\n    kp2, des2 = orb.detectAndCompute(gray_target, None)\n\n    # Match features using BFMatcher with Hamming distance\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n    matches = bf.match(des1, des2)\n    matches = sorted(matches, key=lambda x: x.distance)\n\n    if len(matches) < 4:\n        print(\"Not enough matches found!\")\n        return target_image\n\n    # Extract keypoints for homography\n    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n    # Compute homography matrix\n    H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n\n    # Get object corners and transform them to the target image\n    h, w = gray_object.shape\n    obj_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n    transformed_corners = cv2.perspectiveTransform(obj_corners, H)\n\n    # Extract the bounding box coordinates from transformed corners\n    xmin = int(min(transformed_corners[:, 0, 0]))\n    ymin = int(min(transformed_corners[:, 0, 1]))\n    xmax = int(max(transformed_corners[:, 0, 0]))\n    ymax = int(max(transformed_corners[:, 0, 1]))\n\n    # Draw the bounding box on the target image\n    result_image = target_image.copy()\n    cv2.rectangle(result_image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 3)\n\n    return result_image","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.627768Z","iopub.execute_input":"2024-10-26T04:58:55.628223Z","iopub.status.idle":"2024-10-26T04:58:55.647889Z","shell.execute_reply.started":"2024-10-26T04:58:55.628172Z","shell.execute_reply":"2024-10-26T04:58:55.646786Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def track_multiple_objects(first_image_list, first_mask_list, second_image):\n    \"\"\"\n    Track multiple objects in the second image by matching features and drawing bounding boxes.\n    \n    Parameters:\n    - first_image_list: List of object images extracted from the first set of images.\n    - first_mask_list: List of masks for the corresponding object images.\n    - second_image: The second image where the objects will be searched.\n\n    Returns:\n    - Second image with bounding boxes drawn for all detected objects.\n    \"\"\"\n    result_image = second_image.copy()  # Copy the second image for drawing bounding boxes\n    \n    '''# Loop through each object and its corresponding mask\n    for i in range(len(first_image_list)):\n        print(f\"Tracking object {i + 1}...\")\n\n        # Extract the bounding box for the current object'''\n    object_bbox = extract_bounding_box([first_mask_list[i]])[0]\n\n        # Track the object and get the result image with the bounding box drawn\n    tracked_image = track_object_with_bounding_box(first_image_list[i], result_image)\n        \n   ''' # Display the intermediate results (optional, can be commented out)\n    plt.figure(figsize=(8, 8))\n    plt.imshow(tracked_image)\n    plt.axis('off')\n    plt.show()'''\n        \n    # Update the result image\n    result_image = tracked_image\n    \n    return result_image","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:01:01.548468Z","iopub.status.idle":"2024-10-26T05:01:01.548853Z","shell.execute_reply.started":"2024-10-26T05:01:01.548657Z","shell.execute_reply":"2024-10-26T05:01:01.548679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_object_with_mask(image, mask):\n    \"\"\"\n    Crops the object tightly using the given mask.\n    \n    Parameters:\n    - image (numpy array): The original image.\n    - mask (numpy array): The mask indicating the object area.\n\n    Returns:\n    - Cropped object image (numpy array).\n    \"\"\"\n    # Ensure mask is binary\n    binary_mask = mask.astype(np.uint8) * 255\n\n    # Find contours of the mask to get the tight bounding box\n    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    x, y, w, h = cv2.boundingRect(contours[0])\n\n    # Crop the image using the bounding box\n    cropped_image = image[y:y + h, x:x + w]\n\n    return cropped_image","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.660942Z","iopub.execute_input":"2024-10-26T04:58:55.661384Z","iopub.status.idle":"2024-10-26T04:58:55.671944Z","shell.execute_reply.started":"2024-10-26T04:58:55.661334Z","shell.execute_reply":"2024-10-26T04:58:55.670681Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\nimport os\n\ndef search_jpg_files(directory):\n    \"\"\"\n    Search for all .jpg files in the given directory and its subdirectories.\n\n    Parameters:\n    - directory: Path to the directory to search.\n\n    Returns:\n    - A list of paths to the found .jpg files.\n    \"\"\"\n    jpg_files = []\n\n    # Walk through all subdirectories and files in the given directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.jpg'):  # Check for .jpg extension (case insensitive)\n                jpg_files.append(os.path.join(root, file))\n\n    return jpg_files","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.673373Z","iopub.execute_input":"2024-10-26T04:58:55.673726Z","iopub.status.idle":"2024-10-26T04:58:55.680677Z","shell.execute_reply.started":"2024-10-26T04:58:55.673679Z","shell.execute_reply":"2024-10-26T04:58:55.679650Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def calculate_iou(pred_box, gt_box):\n    \"\"\"\n    Calculate Intersection over Union (IoU) between two bounding boxes.\n\n    Parameters:\n    - pred_box: Tuple (xmin, ymin, xmax, ymax) for predicted bounding box.\n    - gt_box: Tuple (xmin, ymin, xmax, ymax) for ground truth bounding box.\n\n    Returns:\n    - IoU value (float) between 0 and 1.\n    \"\"\"\n    # Calculate intersection coordinates\n    x1 = max(pred_box[0], gt_box[0])\n    y1 = max(pred_box[1], gt_box[1])\n    x2 = min(pred_box[2], gt_box[2])\n    y2 = min(pred_box[3], gt_box[3])\n\n    # Calculate intersection area\n    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n\n    # Calculate areas of predicted and ground truth boxes\n    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n\n    # Calculate union area\n    union_area = pred_area + gt_area - intersection_area\n\n    # Avoid division by zero\n    if union_area == 0:\n        return 0.0\n\n    # Calculate IoU\n    return intersection_area / union_area","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.682208Z","iopub.execute_input":"2024-10-26T04:58:55.682618Z","iopub.status.idle":"2024-10-26T04:58:55.692774Z","shell.execute_reply.started":"2024-10-26T04:58:55.682570Z","shell.execute_reply":"2024-10-26T04:58:55.691703Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def precision_recall(pred_boxes, gt_boxes, iou_threshold=0.5):\n    \"\"\"\n    Calculate precision and recall for predicted and ground truth bounding boxes.\n\n    Parameters:\n    - pred_boxes: List of predicted bounding boxes [(xmin, ymin, xmax, ymax), ...].\n    - gt_boxes: List of ground truth bounding boxes [(xmin, ymin, xmax, ymax), ...].\n    - iou_threshold: IoU threshold to consider a prediction as a true positive.\n\n    Returns:\n    - Precision and recall values (float).\n    \"\"\"\n    tp, fp, fn = 0, 0, 0  # Initialize counters\n    matched = [False] * len(gt_boxes)  # Track matched ground truth boxes\n\n    # Iterate over all predicted boxes\n    for pred_box in pred_boxes:\n        found_match = False\n\n        # Check against all ground truth boxes\n        for i, gt_box in enumerate(gt_boxes):\n            if not matched[i] and calculate_iou(pred_box, gt_box) >= iou_threshold:\n                tp += 1  # True Positive\n                matched[i] = True  # Mark ground truth as matched\n                found_match = True\n                break\n\n        if not found_match:\n            fp += 1  # False Positive\n\n    # Count unmatched ground truth boxes as false negatives\n    fn = matched.count(False)\n\n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    return precision, recall","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.694423Z","iopub.execute_input":"2024-10-26T04:58:55.695829Z","iopub.status.idle":"2024-10-26T04:58:55.705580Z","shell.execute_reply.started":"2024-10-26T04:58:55.695774Z","shell.execute_reply":"2024-10-26T04:58:55.704450Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def calculate_map(pred_boxes_list, gt_boxes_list, iou_threshold=0.5):\n    \"\"\"\n    Calculate Mean Average Precision (mAP) for multiple predictions and ground truths.\n\n    Parameters:\n    - pred_boxes_list: List of predicted boxes for multiple images.\n    - gt_boxes_list: List of ground truth boxes for corresponding images.\n    - iou_threshold: IoU threshold for calculating true positives.\n\n    Returns:\n    - mAP value (float).\n    \"\"\"\n    precisions = []\n    recalls = []\n\n    # Calculate precision and recall for each image\n    for pred_boxes, gt_boxes in zip(pred_boxes_list, gt_boxes_list):\n        precision, recall = precision_recall(pred_boxes, gt_boxes, iou_threshold)\n        precisions.append(precision)\n        recalls.append(recall)\n\n    # Calculate average precision (AP) for the given IoU threshold\n    ap = sum(precisions) / len(precisions) if precisions else 0\n\n    return ap\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:58:55.707159Z","iopub.execute_input":"2024-10-26T04:58:55.707519Z","iopub.status.idle":"2024-10-26T04:58:55.717381Z","shell.execute_reply.started":"2024-10-26T04:58:55.707473Z","shell.execute_reply":"2024-10-26T04:58:55.716360Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Create a directory to store training images\ntraining_images_dir = \"/kaggle/input/cmu-dataset-images/CMU10_3D/modelimages\"\nos.makedirs(training_images_dir, exist_ok=True )\n\n# Load and initialize SAM2 model\ncheckpoint_path = \"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-base-plus/1/sam2_hiera_base_plus.pt\"\nmodel_config = \"sam2_hiera_b+.yaml\"\nsam2_model = build_sam2(model_config, checkpoint_path, device='cuda', apply_postprocessing=False)\nmask_generator = SAM2AutomaticMaskGenerator(sam2_model)\n\n# Load images from the directory and extract objects\nimage_files = os.listdir(training_images_dir)\nimage_paths = [os.path.join(training_images_dir, img) for img in image_files if img.endswith(('.jpg', '.png'))]\n\n# Extract objects from the images in the training directory\nextracted_objects = extract_objects_from_images(image_paths)\n#print(\"Extracted_Objects\",extracted_objects)\n# Display extracted objects\n'''for obj in extracted_objects:\n    plt.figure(figsize=(8, 8))\n    plt.imshow(obj)\n    plt.axis('off')\n    plt.show()'''\n\nimport numpy as np\n# Load the first image and mask\npath = \"/kaggle/input/cmu-dataset-images/CMU10_3D/modelimages/\"\nobjects = ['can_chowder/can_chowder_01.jpg','can_chowder/can_chowder_02.jpg','can_chowder/can_chowder_03.jpg','can_chowder/can_chowder_04.jpg','can_chowder/can_chowder_05.jpg',\n          'can_soymilk/can_soymilk_01.jpg','can_soymilk/can_soymilk_02.jpg','can_soymilk/can_soymilk_03.jpg','can_soymilk/can_soymilk_04.jpg','can_soymilk/can_soymilk_05.jpg',\n          'can_tomatosoup/can_tomatosoup_01.jpg','can_tomatosoup/can_tomatosoup_02.jpg','can_tomatosoup/can_tomatosoup_03.jpg','can_tomatosoup/can_tomatosoup_04.jpg','can_tomatosoup/can_tomatosoup_05.jpg',\n          'carton_oj/carton_oj_01.jpg','carton_oj/carton_oj_02.jpg','carton_oj/carton_oj_03.jpg','carton_oj/carton_oj_04.jpg','carton_oj/carton_oj_05.jpg',\n          'carton_soymilk/carton_soymilk_01.jpg','carton_soymilk/carton_soymilk_02.jpg','carton_soymilk/carton_soymilk_03.jpg','carton_soymilk/carton_soymilk_04.jpg','carton_soymilk/carton_soymilk_05.jpg',\n          'diet_coke/diet_coke_01.jpg','diet_coke/diet_coke_02.jpg','diet_coke/diet_coke_03.jpg','diet_coke/diet_coke_04.jpg','diet_coke/diet_coke_05.jpg',\n          'hc_potroastsoup/hc_potroastsoup_01.jpg','hc_potroastsoup/hc_potroastsoup_02.jpg','hc_potroastsoup/hc_potroastsoup_03.jpg','hc_potroastsoup/hc_potroastsoup_04.jpg','hc_potroastsoup/hc_potroastsoup_05.jpg',\n          'juicebox/juicebox_01.jpg','juicebox/juicebox_02.jpg','juicebox/juicebox_03.jpg','juicebox/juicebox_04.jpg','juicebox/juicebox_05.jpg',\n           'rice_tuscan/rice_tuscan_01.jpg','rice_tuscan/rice_tuscan_02.jpg','rice_tuscan/rice_tuscan_03.jpg','rice_tuscan/rice_tuscan_04.jpg','rice_tuscan/rice_tuscan_05.jpg',\n           'ricepilaf/ricepilaf_01.jpg','ricepilaf/ricepilaf_02.jpg','ricepilaf/ricepilaf_03.jpg','ricepilaf/ricepilaf_04.jpg','ricepilaf/ricepilaf_05.jpg']\nobject_png = [obj.replace('.jpg', '.png') for obj in objects]\nfirst_image_list = []\nfirst_mask_list = []\nobject_cropped = []\n\ngt_boxes_list = []\npred_boxes_list = []\nfor i in objects:\n    #print(path+i)\n    first_image_list.append(np.array(Image.open(path+i).convert(\"RGB\")))\nfor i in object_png:\n    first_mask_list.append(np.array(Image.open(path+i)))\n\n# Crop the object tightly from the first image\nfor i in range(len(objects)):\n    object_cropped = crop_object_with_mask(first_image_list[i], first_mask_list[i])\n    gt_boxes_list.append(object_cropped)\n\n        \n#Example usage:\ndirectory_path = '/kaggle/input/cmu-dataset-images/CMU10_3D/data_2D'  # Replace with your target directory path\njpg_files = search_jpg_files(directory_path)\n            \n# Load the second image where the object will be searched\nsearch_path = \"/kaggle/input/cmu-dataset-images/CMU10_3D/data_2D\"\n\nfor jpg in range(len(jpg_files)//4):\n    second_image = np.array(Image.open(jpg_files[jpg]).convert(\"RGB\"))\n    #print(\"Second_image\",second_image)\n    # Extract the bounding box from the first image's mask\n    object_bbox = extract_bounding_box(first_mask_list)\n    pred_boxes_list.append(object_bbox)  # Assuming bbox is already in format [x_min, y_min, x_max, y_max]\n    # Track and highlight the object in the second image\n    final_tracked_image = track_multiple_objects(first_image_list, first_mask_list, second_image)\n    tracked_image = track_object_with_bounding_box(first_image_list[i], second_image)\n    filename = jpg_files[jpg]        \n    save_image(tracked_image, filename)\n    '''plt.figure(figsize=(10, 10))\n    plt.imshow(final_tracked_image)\n    plt.axis('off')\n    plt.show()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate_map(pred_boxes_list, gt_boxes_list, iou_threshold=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:01:01.546289Z","iopub.status.idle":"2024-10-26T05:01:01.546720Z","shell.execute_reply.started":"2024-10-26T05:01:01.546510Z","shell.execute_reply":"2024-10-26T05:01:01.546533Z"},"trusted":true},"execution_count":null,"outputs":[]}]}